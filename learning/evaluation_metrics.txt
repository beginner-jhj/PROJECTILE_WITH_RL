| 지표 이름             | 의미         | 설명                                                                                     |
| ----------------- | ---------- | -------------------------------------------------------------------------------------- |
| `ep_len_mean`     | 평균 에피소드 길이 | 한 에피소드가 평균적으로 몇 step 만에 끝났는지. 값이 1이면 한 번에 종료된다는 뜻.                    |
| `ep_rew_mean`     | 평균 리워드     | 각 에피소드의 총 reward 평균. 클수록 좋은 정책. |
| `fps`             | 초당 프레임 수   | 초당 처리 가능한 시뮬레이션 횟수. CPU 속도와 환경 복잡도에 따라 달라짐.                                            |
| `iterations`      | 학습 반복 횟수   | 얼마나 많은 학습 주기가 진행되었는지 (보통 2048 step 단위).                                                |
| `time_elapsed`    | 총 경과 시간(초) | 학습 시작 후 얼마나 시간이 흘렀는지.                                                                  |
| `total_timesteps` | 전체 step 수  | 지금까지 학습에 사용된 총 환경 상호작용 수. `iterations * 2048`과 비슷함.                                    |



| 지표 이름                  | 의미                 | 설명                                                                |
| ---------------------- | ------------------ | ----------------------------------------------------------------- |
| `approx_kl`            | KL 발산 추정치          | 이전 정책과 새 정책 간의 차이. 너무 크면 학습이 불안정, 너무 작으면 학습이 느림. (0.01\~0.1이 적당함) |
| `clip_fraction`        | gradient clip 비율   | PPO는 큰 gradient 업데이트를 잘라냄. 이 비율이 높으면 정책이 크게 바뀌려고 한 것.             |
| `clip_range`           | 클리핑 한계             | PPO가 정책을 얼마나 제한할지의 기준. 일반적으로 0.2.                                 |
| `entropy_loss`         | 정책의 무작위성           | 높을수록 행동이 다양함. 너무 낮으면 정책이 고정되어 탐험 부족. (-1.4이면 거의 한 행동만 고수 중)       |
| `explained_variance`   | value function 예측력 | 0이면 예측 불가, 1이면 완벽 예측. 낮으면 value loss가 잘 학습되지 않는 것.                |
| `learning_rate`        | 학습률                | 현재 사용 중인 optimizer의 학습률.                                          |
| `loss`                 | 전체 loss            | value loss + policy loss 등 포함된 전체 손실 값.                           |
| `n_updates`            | 업데이트 수             | 지금까지 네트워크를 업데이트한 횟수.                                              |
| `policy_gradient_loss` | 정책 학습 손실           | 정책을 개선하는 데 사용된 gradient의 크기. 0이면 학습이 거의 없음.                       |
| `std`                  | 정책 분포의 표준편차        | 행동의 다양성. 클수록 행동 선택이 분산됨. 낮으면 같은 행동만 선택.                           |
| `value_loss`           | 가치 함수 손실           | 현재 value network가 에피소드의 보상을 얼마나 잘 예측하는지. 낮을수록 정확함.                |
